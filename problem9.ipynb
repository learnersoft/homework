{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "DATA_DIR = 'D:/enron'\n",
    "target_names = ['ham', 'spam']\n",
    "def get_data(DATA_DIR):\n",
    "    subfolders = ['enron%d' % i for i in range(1,7)]\n",
    "    data = []\n",
    "    target = []\n",
    "    for subfolder in subfolders:\n",
    "        # spam\n",
    "        spam_files = os.listdir(os.path.join(DATA_DIR, subfolder, 'spam'))\n",
    "        for spam_file in spam_files:\n",
    "            with open(os.path.join(DATA_DIR, subfolder, 'spam', spam_file), encoding=\"latin-1\") as f:\n",
    "                data.append(f.read())\n",
    "                target.append(1)\n",
    "        # ham\n",
    "        ham_files = os.listdir(os.path.join(DATA_DIR, subfolder, 'ham'))\n",
    "        for ham_file in ham_files:\n",
    "            with open(os.path.join(DATA_DIR, subfolder, 'ham', ham_file), encoding=\"latin-1\") as f:\n",
    "                data.append(f.read())\n",
    "                target.append(0)\n",
    "    return data, target\n",
    " \n",
    "X, y = get_data(DATA_DIR)\n",
    " \n",
    "class SpamDetector_1(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "    #清除空格\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "    #分开每个单词\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "    #计算某个单词出现的次数\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    " \n",
    "class SpamDetector_2(SpamDetector_1):\n",
    "    # X:data,Y:target标签（垃圾邮件或正常邮件）\n",
    "    def fit(self, X, Y):\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        # 建立一个集合存储所有出现的单词\n",
    "        self.vocab = set()\n",
    "        # 统计spam和ham邮件的个数\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 1)\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 0)\n",
    " \n",
    "        # 计算先验概率，即所有的邮件中，垃圾邮件和正常邮件所占的比例\n",
    "        self.log_class_priors['spam'] = math.log(\n",
    "            self.num_messages['spam'] / (self.num_messages['spam'] + self.num_messages['ham']))\n",
    "        self.log_class_priors['ham'] = math.log(\n",
    "            self.num_messages['ham'] / (self.num_messages['spam'] + self.num_messages['ham']))\n",
    " \n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    " \n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 1 else 'ham'\n",
    "            # 构建一个字典存储单封邮件中的单词以及其个数\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)#确保self.vocab中含有所有邮件中的单词\n",
    "                # 下面语句是为了计算垃圾邮件和非垃圾邮件的词频，即给定词在垃圾邮件和非垃圾邮件中出现的次数。\n",
    "                # c是0或1，垃圾邮件的标签\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "                self.word_counts[c][word] += count\n",
    " \n",
    "MNB = SpamDetector_2()\n",
    "MNB.fit(X[100:], y[100:])\n",
    " \n",
    "class SpamDetector(SpamDetector_2):\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        flag_1 = 0\n",
    "        # 遍历所有的测试集\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))  # 生成可以记录单词以及该单词出现的次数的字典\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            flag_2 = 0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    " \n",
    "                #下面计算P(内容|垃圾邮件)和P(内容|正常邮件),所有的单词都要进行拉普拉斯平滑\n",
    "                else:\n",
    "                    # 该单词存在于正常邮件的训练集和垃圾邮件的训练集当中\n",
    "                    if word in self.word_counts['spam'].keys() and word in self.word_counts['ham'].keys():\n",
    "                        log_w_given_spam = math.log(\n",
    "                            (self.word_counts['spam'][word] + 1) / (sum(self.word_counts['spam'].values()) + len(self.vocab)))\n",
    "                        log_w_given_ham = math.log(\n",
    "                            (self.word_counts['ham'][word] + 1) / (sum(self.word_counts['ham'].values()) + len(\n",
    "                                self.vocab)))\n",
    "                    # 该单词存在于垃圾邮件的训练集当中,但不存在于正常邮件的训练集当中\n",
    "                    if word in self.word_counts['spam'].keys() and word not in self.word_counts['ham'].keys():\n",
    "                        log_w_given_spam = math.log(\n",
    "                            (self.word_counts['spam'][word] + 1) / (sum(self.word_counts['spam'].values()) + len(self.vocab)))\n",
    "                        log_w_given_ham = math.log( 1 / (sum(self.word_counts['ham'].values()) + len(\n",
    "                                self.vocab)))\n",
    "                    # 该单词存在于正常邮件的训练集当中,但不存在于垃圾邮件的训练集当中\n",
    "                    if word not in self.word_counts['spam'].keys() and word in self.word_counts['ham'].keys():\n",
    "                        log_w_given_spam = math.log( 1 / (sum(self.word_counts['spam'].values()) + len(self.vocab)))\n",
    "                        log_w_given_ham = math.log(\n",
    "                            (self.word_counts['ham'][word] + 1) / (sum(self.word_counts['ham'].values()) + len(\n",
    "                                self.vocab)))\n",
    " \n",
    "                # 把计算到的P(内容|垃圾邮件)和P(内容|正常邮件)加起来\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    " \n",
    "                flag_2 += 1\n",
    " \n",
    "                # 最后，还要把先验加上去，即P(垃圾邮件)和P(正常邮件)\n",
    "                spam_score += self.log_class_priors['spam']\n",
    "                ham_score += self.log_class_priors['ham']\n",
    " \n",
    "            # 最后进行预测，如果spam_score > ham_score则标志为1，即垃圾邮件\n",
    "            if spam_score > ham_score:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    " \n",
    "            flag_1 += 1\n",
    " \n",
    "        return result\n",
    " \n",
    "MNB = SpamDetector()\n",
    "MNB.fit(X[100:], y[100:])\n",
    "pred = MNB.predict(X[:100])\n",
    "true = y[:100]\n",
    " \n",
    "accuracy = 0\n",
    "for i in range(100):\n",
    "    if pred[i] == true[i]:\n",
    "        accuracy += 1\n",
    "print(accuracy) # 0.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
